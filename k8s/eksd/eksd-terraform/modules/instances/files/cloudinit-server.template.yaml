#cloud-config
write_files:
  - path: /etc/kubernetes/zadara/values-aws-cloud-controller.yaml
    owner: root:root
    permissions: "0644"
    content: |
      args:
        - --v=2
        - --cloud-provider=aws
        - --cloud-config=config/cloud.conf
        - --allocate-node-cidrs=false
        - --cluster-cidr=${ pod_network }
        - --cluster-name=${ cluster_name }
        - --configure-cloud-routes=false
      image:
          tag: "CCM_VER"
      cloudConfigPath: config/cloud.conf
      extraVolumes:
        - name: cloud-config
          configMap:
            name: cloud-config
        - name: trusted-root-cas
          hostPath:
            path: /etc/ssl/certs/ca-certificates.crt
            type: File
      extraVolumeMounts:
        - name: cloud-config
          mountPath: config
        - name: trusted-root-cas
          mountPath: /etc/ssl/certs/zadara-ca.crt
  - path: /etc/kubernetes/zadara/etcd-backup.sh
    owner: root:root
    permissions: "0755"
    content: |
      #!/bin/bash

      echo "`date`: ETCD backup - started"

      BACKUP_ROTATION=${ backup_rotation } # multi-masters parallelism will likely make it a bit lower
      if [ ! $BACKUP_ROTATION -gt 0 ]; then
          echo "`date`: ETCD backup - disabled per non-positive rotation value"
          exit 0
      fi
      
      api_endpoint="$(curl -s http://169.254.169.254/openstack/latest/meta_data.json | jq -c '.cluster_url' | cut -d\" -f2)"

      shopt -s expand_aliases
      alias kubectl="kubectl --kubeconfig=/etc/kubernetes/admin.conf --namespace kube-system"
      alias ec2="aws ec2 --endpoint-url $api_endpoint/api/v2/aws/ec2"

      host_name="$(hostname)"
      host_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
      cluster_name="${ cluster_name }"
      cluster_certificate="${ certificate }" # the only "real" unique cluster identifier
      cluster_id=$${cluster_certificate:0:7}
      local_filename='etcd_backup_'$host_name'_'$host_id'.db'

      echo "`date`: ETCD backup - saving locally to /etc/kubernetes/zadara/$local_filename"
      ETCDCTL_API=3 etcdctl \
          --endpoints=https://localhost:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --key=/etc/kubernetes/pki/etcd/peer.key \
          --cert=/etc/kubernetes/pki/etcd/peer.crt \
          snapshot save "/etc/kubernetes/zadara/$local_filename"

      echo "`date`: ETCD backup - snapshotting boot volume"
      volume=$(ec2 describe-volumes --filters Name=attachment.instance-id,Values=$host_id --query 'Volumes[0].VolumeId' | cut -d\" -f2)
      ec2 create-snapshot \
          --volume-id $volume \
          --description "Automated snapshot of EKS-D control-plane boot drive (containing ETCD)" \
          --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=EKS-D_autosnap_'$host_name'},{Key=managed-by,Value='$cluster_name'}]' \
          --output text
      if [ $? -ne 0 ]; then
          echo "`date`: ETCD backup - error while snapshotting (run manually to see the exact issue), exiting with error"
          exit 1
      fi

      while [ $(ec2 describe-snapshots --filter "Name=tag:managed-by,Values='$cluster_name'" --query 'Snapshots[*].SnapshotId | length(@)') -gt $BACKUP_ROTATION ]
      do
          oldest=$(ec2 describe-snapshots --filter "Name=tag:managed-by,Values='$cluster_name'" --query 'sort_by(Snapshots, &StartTime)[0].SnapshotId' | cut -d\" -f2)
          echo "`date`: ETCD backup - attempting to purge snapshot $oldest"
          ec2 delete-snapshot --snapshot-id $oldest
      done

      secret=$(kubectl get secret zadara-backup-export -o name)
      if [ -n "$secret" ]; then
          export AWS_ACCESS_KEY_ID="$(kubectl get secret zadara-backup-export -o jsonpath='{.data.backup_access_key_id}' | base64 -d)"
          export AWS_SECRET_ACCESS_KEY="$(kubectl get secret zadara-backup-export -o jsonpath='{.data.backup_secret_access_key}' | base64 -d)"
          export AWS_REGION="$(kubectl get secret zadara-backup-export -o jsonpath='{.data.backup_region}' | base64 -d)"
          export AWS_ENDPOINT_URL="$(kubectl get secret zadara-backup-export -o jsonpath='{.data.backup_endpoint}' | base64 -d)"
          BUCKET="$(kubectl get secret zadara-backup-export -o jsonpath='{.data.backup_bucket}' | base64 -d)"
      else
          export AWS_ACCESS_KEY_ID="${ backup_access_key_id }"
          export AWS_SECRET_ACCESS_KEY="${ backup_secret_access_key }"
          export AWS_REGION="${ backup_region }"
          export AWS_ENDPOINT_URL="${ backup_endpoint }"
          BUCKET="${ backup_bucket }"
      fi

      if [ -z "$(echo $AWS_ENDPOINT_URL)" ]; then
          unset AWS_ENDPOINT_URL  #real AWS S3 will not use endpoint
      fi

      if [[ -z "$(echo $AWS_ACCESS_KEY_ID)" || -z "$(echo $AWS_SECRET_ACCESS_KEY)" || -z "$(echo $AWS_REGION)" || -z "$BUCKET" ]]; then
          echo "`date`: ETCD backup - finished (without export - skipped due to missing S3 information)"
          exit 0
      fi
      timestamp=$(date +%y-%m-%d_%H-%M-%S)
      remote_filename="$timestamp"_"$local_filename"
      remote_prefix="$cluster_name"_"$cluster_id"

      echo "`date`: ETCD backup - exporting to $BUCKET/$remote_prefix/$remote_filename"
      aws s3 cp /etc/kubernetes/zadara/$local_filename s3://$BUCKET/$remote_prefix/$remote_filename
      if [ $? -ne 0 ]; then
          echo "`date`: ETCD backup - error while exporting (run manually to see the exact issue), exiting with error"
          exit 1
      fi

      while [ $(aws s3 ls --recursive s3://$BUCKET/$remote_prefix/ | wc -l) -gt $BACKUP_ROTATION ]
      do
          oldest=$(aws s3api list-objects-v2 --bucket $BUCKET --prefix $remote_prefix --query 'sort_by(Contents, &LastModified)[0].Key' | cut -d\" -f2)
          echo "`date`: ETCD backup - attempting to purge $BUCKET/$remote_prefix/$oldest"
          aws s3 rm s3://$BUCKET/$oldest
      done

      echo "`date`: ETCD backup - finished (with export)"
  - path: /etc/kubernetes/zadara/etcd-watchdog.sh
    owner: root:root
    permissions: "0755"
    content: |
      #!/bin/bash

      echo "`date`: ETCD watchdog: monitoring unhealthy endpoints"

      shopt -s expand_aliases
      alias etcdctl="ETCDCTL_API=3 etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/etcd/peer.key --cert=/etc/kubernetes/pki/etcd/peer.crt"

      bad_endpoints=$(etcdctl endpoint health --cluster -w json | jq -r '.[] | select(.health==false) | .endpoint')
      if [[ -z "$bad_endpoints" || "$bad_endpoints" = "null" ]]; then
          echo "`date`: ETCD watchdog: no unhealthy endpoints found (good!) - existing"
          exit 0
      fi

      for endpoint in $bad_endpoints; do
          privateip="$(echo $endpoint | cut -d/ -f3 | cut -d: -f1)"
          
          echo "`date`: ETCD watchdog: found bad ETCD endpoint ($endpoint) - searching instances with ip $privateip"
          api_endpoint="$(curl -s http://169.254.169.254/openstack/latest/meta_data.json | jq -c '.cluster_url' | cut -d\" -f2)"
          instances="$(aws ec2 --endpoint-url $api_endpoint/api/v2/aws/ec2 describe-instances --output json)"
          if [ $? -ne 0 ]; then
              echo "`date`: ETCD watchdog: error getting instances from AWS EC2 API - existing with error"
              exit 1
          fi
          if grep -q "$privateip" <<< "$instances"; then
              echo "`date`: ETCD watchdog: $privateip actually exist in instances - exiting without removing (might be a temporary issue)"
              exit 0
          fi
          echo "`date`: ETCD watchdog: $privateip was not found within any instance - candidate for ETCD member removal"
          member=$(etcdctl member list -w json --hex=true | jq -r --arg endpoint $endpoint '.members[] | select(.clientURLS | contains([$endpoint])) | "\(.name) \(.ID)"')
          name=$(echo "$member" | cut -d ' ' -f 1)
          id=$(echo "$member" | cut -d ' ' -f 2)
          if [[ -z "$name" || -z "$id" ]]; then
              echo "`date`: ETCD watchdog: no member information for $endpoint - existing with error"
              exit 1
          fi
          echo "`date`: ETCD watchdog: removing ETCD member by id $id ($name) from ETCD"
          etcdctl member remove $id
      done

      echo "`date`: ETCD watchdog: finished"
  - path: /etc/cron.d/eksd-reload-certs
    owner: root:root
    permissions: "0644"
    content: "0 */1 * * * root kubeadm init phase upload-certs --upload-certs --certificate-key ${ certificate } \n"
  - path: /etc/cron.d/eksd-etcd-backup
    owner: root:root
    permissions: "0644"
    content: "0 */2 * * * root /etc/kubernetes/zadara/etcd-backup.sh >> /etc/kubernetes/zadara/etcd-backup.log \n"
  - path: /etc/cron.d/eksd-etcd-watchdog
    owner: root:root
    permissions: "0644"
    content: "*/2 * * * * root /etc/kubernetes/zadara/etcd-watchdog.sh >> /etc/kubernetes/zadara/etcd-watchdog.log \n"
  - path: /etc/kubernetes/zadara/kubeadm-config.yaml
    owner: root:root
    permissions: "0644"
    content: |
      apiVersion: kubeadm.k8s.io/v1beta3
      kind: InitConfiguration
      bootstrapTokens:
        - token: "${ token }"
          description: "predefined kubeadm bootstrap token that never expires"
          ttl: "0"
      certificateKey: "${ certificate }"
      ---
      apiVersion: kubeadm.k8s.io/v1beta3
      kind: ClusterConfiguration
      networking:
        podSubnet: "${ pod_network }"
      kubernetesVersion: "KUBE_VER"
      controlPlaneEndpoint: "${ server_url }"
      etcd:
        local:
          imageRepository: "public.ecr.aws/eks-distro/etcd-io"
          imageTag: "ETCD_VER"
          dataDir: "/var/lib/etcd"
          extraArgs:
            max-snapshots: "60"
      dns:
        imageRepository: "public.ecr.aws/eks-distro/coredns"
        imageTag: "DNS_VER"
      apiServer:
        certSANs:
          %{~ for single_san in san ~}
          - ${ single_san }
          %{~ endfor ~}
      imageRepository: "public.ecr.aws/eks-distro/kubernetes"
      clusterName: "${ cluster_name }"
  - path: /usr/bin/mount_nvme.sh
    owner: root:root
    permissions: '0755'
    content: |
      #!/bin/bash
      set -x
      NVME_PREFIX=$${NVME_PREFIX:-"nvme-SAMSUNG_"}
      MOUNT_PATH=$${MOUNT_PATH:-"/nvme"}
      INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)

      function clean_mount_nvme() {
        mkfs.xfs -f $1
        mkdir -p $2
        echo "mount the $1 as $2"
        mount $1 $2 -t xfs -o defaults

        echo "change $2 file mode to 777"
        chmod 777 $2
        touch $2/$INSTANCE_ID
      }

      function real_mount() {
        mkdir -p $2
        echo "mount the $1 as $2"
        mount $1 $2 -t xfs -o defaults
        rc=$?
        if [ $rc -ne 0 ] || [ ! -f $2/$INSTANCE_ID ]; then
          umount $2
          clean_mount_nvme $1 $2
        fi
      }

      NVMES=$(find /dev/disk/by-id -name "$NVME_PREFIX*_1" ! -name "*part*")
      IDX=1

      for DEV in $NVMES; do
        path=$MOUNT_PATH$IDX
        umount $path
        real_mount $DEV $path
        IDX=$(($IDX + 1))
      done
  - path: /usr/lib/systemd/system/mount-nvme.service
    owner: root:root
    permissions: '0644'
    content: |
      [Unit]
      Description=mount-nvme Service
      After=network.service

      [Service]
      Type=oneshot
      ExecStart=/usr/bin/mount_nvme.sh
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target
runcmd:
  - /usr/sbin/update-ca-certificates
  - systemctl restart containerd
  - apt-get install -y nfs-common
  - systemctl daemon-reload
  - systemctl enable mount-nvme.service
  - systemctl start --no-block mount-nvme.service
ca_certs:
  trusted: |
    ${indent(4, root_ca_cert)}
